{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMblnPgI4pQh"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this assignment you will practice putting together an image classification pipeline based on CNNs for [CIFAR-10 and/or CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. The goals of this assignment are as follows:\n",
    "\n",
    "\n",
    "\n",
    "*   Understand the components of a CNN model and a Vision Transformer (ViT) model.\n",
    "*   Understand how to modify a standard CNN model towards a specific task.\n",
    "*   Implement a basic neural network training pipeline in Pytorch.\n",
    "*   Implement and train an AlexNet model.\n",
    "*   Implement and train a ResNet model.\n",
    "*   Implement and train a ViT model.\n",
    "*   Understand the differences and tradeoffs between these models.\n",
    "\n",
    "Please fill in all the **TODO** code blocks. Once you are ready to submit:\n",
    "\n",
    "* Export the notebook `CSCI677_assignment_3.ipynb` as a PDF `[Your USC ID]_CSCI677_assignment_3.pdf`\n",
    "\n",
    "Please make sure that the notebook have been run before exporting PDF, and your code and all cell outputs are visible in your submitted PDF. Regrading request will not be accepted if your code/output is not visible in the original submission. Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3_vNZ33-kcr"
   },
   "source": [
    "In case you haven't installed PyTorch yet, run the following command to install torch and torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhaHm9fe-vYH"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "True\n",
      "NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AuYeTAzEoGaM"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ucCCRiq6BVp"
   },
   "source": [
    "# **Data Preparation**\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) is a well known dataset composed of 60,000 colored 32x32 images in 10 classes, with 6000 images per class. The utility function `cifar10()` returns the entire CIFAR-10 dataset as a set of four Torch tensors:\n",
    "* `x_train` contains all training images (real numbers in the range  [0,1] )\n",
    "* `y_train` contains all training labels (integers in the range  [0,9] )\n",
    "* `x_test` contains all test images\n",
    "* `y_test` contains all test labels\n",
    "\n",
    "This function automatically downloads the CIFAR-10 dataset the first time you run it.\n",
    "\n",
    "[CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) is just like the CIFAR-10 dataset, except it has 100 classes containing 600 images each. Below we provided wrapper classes for CIFAR-10 and CIFAR-100 datasets. You can choose one or both of them for training your CNNs. If you choose one of them, use the same one to train all your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qh-a1XGO9qNF"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CIFAR10Dataset:\n",
    "    def __init__(self, batch_size=128, root=\"data\"):\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))]\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.training_data = datasets.CIFAR10(\n",
    "            root=root,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=self.transform\n",
    "        )\n",
    "        self.train_dataloader = DataLoader(self.training_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.test_data = datasets.CIFAR10(\n",
    "            root=root,\n",
    "            train=False,\n",
    "            download=False,\n",
    "            transform=self.transform\n",
    "        )\n",
    "        self.test_dataloader = DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        self.classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "class CIFAR100Dataset:\n",
    "    def __init__(self, batch_size=128, root=\"data\"):\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))]  # CIFAR-100 normalization values\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.training_data = datasets.CIFAR100(\n",
    "            root=root,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=self.transform\n",
    "        )\n",
    "        self.train_dataloader = DataLoader(self.training_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.test_data = datasets.CIFAR100(\n",
    "            root=root,\n",
    "            train=False,\n",
    "            download=False,\n",
    "            transform=self.transform\n",
    "        )\n",
    "        self.test_dataloader = DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        self.classes = self.training_data.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7wvdgCHoGaO",
    "outputId": "e680861e-708d-4e3b-89ec-882648e5a670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 22\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of trainable parameters in a model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage\n",
    "model = torch.nn.Linear(10, 2)  # Example model\n",
    "print(f\"Number of parameters: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZalEn-ux99oz"
   },
   "source": [
    "# AlexNet (20 pts)\n",
    "AlexNet, introduced by Alex Krizhevsky in 2012, marked a significant breakthrough in deep learning for computer vision. This deep convolutional neural network consists of five convolutional layers, some followed by max-pooling layers, and three fully connected layers. AlexNet was designed for large-scale image classification tasks and was notably successful in the ImageNet Large Scale Visual Recognition Challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2nZE65W_PBU"
   },
   "source": [
    "## Implement AlexNet (20 pts)\n",
    "Classical AlexNet architecture is as follows:\n",
    "\n",
    "\n",
    "![LeNet-5 Architecture](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*wgJ9iOjl_JzjOZ3e9jDFAw.png)\n",
    "\n",
    "\n",
    "The original AlexNet was designed for high-resolution images (224x224x3) from the ImageNet dataset. However, the CIFAR-10 and CIFAR-100 datasets consist of lower-resolution images (32x32x3). To adapt AlexNet for these datasets, you need to modify it.\n",
    "\n",
    "Requirements:\n",
    "* **Input Adaptation**: Modify the network to accept 32x32x3 input dimensions, suitable for CIFAR-10 and CIFAR-100 images.\n",
    "* **Architecture**: Implement a network with the following layers:\n",
    "\n",
    "  (Convolutional Layer 1 -> ReLU -> Max Pooling 1) ->\n",
    "\n",
    "  (Convolutional Layer 2 -> ReLU -> Max Pooling 2) ->\n",
    "\n",
    "  (Convolutional Layer 3 -> ReLU -> Convolutional Layer 4 -> ReLU -> Convolutional Layer 5 -> ReLU -> Max Pooling 3) ->\n",
    "\n",
    "  Flattening ->\n",
    "\n",
    "  (Linear -> ReLU) ->\n",
    "\n",
    "  (Linear -> ReLU) -> Linear.\n",
    "* Use you can design your own convolution filters and max pooling layers.\n",
    "* Your model must contains less than **40 Million** parameters. We provide `count_parameters()` function to count the number of parameters in a model.\n",
    "\n",
    "**Hint**: you can use nn.Sequential() to simplify your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LdieMH2P9U1W"
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        # TODO\n",
    "        self.features = nn.Sequential(\n",
    "            # Input 32x32, conv, then downsample to 16x16\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),   # Output: 64 x 32 x 32\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                    # Output: 64 x 16 x 16\n",
    "\n",
    "            # Input 32x32, conv, then downsample to 8x8\n",
    "            nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1),   # Output: 192 x 16 x 16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                    # Output: 192 x 8 x 8\n",
    "\n",
    "            # Input 8x8, 3 convs, then downsample to 4x4\n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),  # Output: 384 x 8 x 8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),  # Output: 256 x 8 x 8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # Output: 256 x 8 x 8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)                     # Output: 256 x 4 x 4\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 4096),\n",
    "            nn.ReLU(inplace=True), # inplace = True to save memory\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the 4x4 conv output for the classifier\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4o3qwInhsnT"
   },
   "source": [
    "# ResNet (20 pts)\n",
    "ResNet, short for Residual Network, was introduced in 2015 by Kaiming He et al. At its core, ResNet introduces the concept of residual blocks, which allows gradients to flow directly through the network's many layers. In comparison to earlier architectures like AlexNet, ResNet's approach demonstrates the transformative power of residual connections.\n",
    "\n",
    "In this section, you will implement ResNet-18 for CIFAR-10/100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LQbwuLUj1mO"
   },
   "source": [
    "## Implement Residual Block (10 pts)\n",
    "The Residual Block is a crucial component in ResNet. It works by introducing a shortcut connection, also known as a skip connection, alongside a regular neural network layer. This shortcut connection enables the flow of information directly from one layer to another, bypassing some intermediate layers.\n",
    "\n",
    "The key idea is to learn a residual function, which represents the difference between the desired output and the current output of the block. By doing so, the block aims to make the output closer to what it should be. This approach mitigates the vanishing gradient problem, which can occur in very deep networks, making it easier to train deep models effectively.\n",
    "\n",
    "![Residual Block](https://miro.medium.com/v2/resize:fit:1140/format:webp/1*6WlIo8W1_Qc01hjWdZy-1Q.png)\n",
    "\n",
    "\n",
    "The weight layer usually consists of a convolutional layer and a batch normalization layer. The batch normalization layer, often abbreviated as BatchNorm, normalizes the input of a neural network layer across a mini-batch of data during training. BatchNorm not only accelerates convergence but also acts as a form of regularization, reducing the risk of overfitting. In PyTorch, it is implemented by nn.BatchNorm2d().\n",
    "\n",
    "You are asked to implement the residual block with the following requirements:\n",
    "* The residual block takes input of size n * n * `in_channels` and output m * m * `out_channels` with m = (n-1) / `stride` + 1\n",
    "* The residual function consists of the following components:\n",
    "\n",
    "  Conv -> BatchNorm -> ReLU -> Conv -> BatchNorm\n",
    "\n",
    "  where Conv means 3x3 convolutional filters with padding 1. If `stride` != 1, set stride for the first Conv.\n",
    "* The shortcut should be identity if `in_channels` == `out_channels` and `stride` == 1. Otherwise, it should be a convolutional layer with kernel_size=1 and stride=`stride`.\n",
    "* After adding the residual function and the shortcut, apply another ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "56act4Fdtbxl"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # TODO\n",
    "        # Residual function: Conv -> BN -> ReLU -> Conv -> BN\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False) # set bias to False to avoid redundancy bc BatchNorm has its own\n",
    "        self.bn1   = nn.BatchNorm2d(out_channels) # Input size = output size from conv1\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Shortcut connection: identity if input and output dimensions match; otherwise 1x1 conv with BN to match them\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        # Residual function: Conv -> BN -> ReLU -> Conv -> BN\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # Add the shortcut connection\n",
    "        out += self.shortcut(x)\n",
    "        # Final ReLU activation for residual block\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RYB3dvEzavj"
   },
   "source": [
    "## Implement ResNet (10 pts)\n",
    "ResNet-18 is part of the ResNet family, known for its exceptional depth and performance in image classification tasks. It consists of 18 layers, beginning with one convolutional layer, followed by a few residual blocks, and ending with a fully-connected layer. Here is a glimpse of its architecture:\n",
    "\n",
    "\n",
    "![ResNet-18](https://www.researchgate.net/profile/Sajid-Iqbal-13/publication/336642248/figure/fig1/AS:839151377203201@1577080687133/Original-ResNet-18-Architecture.png)\n",
    "\n",
    "\n",
    "In this part of the assignment, you are asked to implement a modified ResNet for CIFAR-10/100. Requirements:\n",
    "* The model should take inputs of 32x32x3 and output a vector of dimension equal to the number of classes (10 for CIFAR-10 and 100 for CIFAR-100).\n",
    "* The model should begin with a convolutional layer with kernel_size=3 and padding=1:\n",
    "\n",
    "  Conv -> BatchNorm -> ReLU\n",
    "\n",
    "  The output size should be 64x32x32.\n",
    "* After the first layer, append with 4 residual blocks such that the output size changes as follows:\n",
    "  \n",
    "  (Input size after previous step) 64x32x32 -> 64x32x32 -> 256x16x16 -> 256x8x8 -> 512x2x2\n",
    "* The model should end with average pooling (kernel_size=2), flattening, and a fully-connected layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "znZV9y734trg"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        # TODO\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 64x32x32 -> 64x32x32 -> 256x16x16 -> 256x8x8 -> 512x2x2\n",
    "\n",
    "        # Block 1: 64x32x32 -> 64x32x32 (no downsampling)\n",
    "        self.layer1 = ResidualBlock(64, 64, stride=1)\n",
    "        # Block 2: 64x32x32 -> 256x16x16 (downsample spatially and increase channels)\n",
    "        self.layer2 = ResidualBlock(64, 256, stride=2)\n",
    "        # Block 3: 256x16x16 -> 256x8x8 (downsample spatially)\n",
    "        self.layer3 = ResidualBlock(256, 256, stride=2)\n",
    "        # Block 4: 256x8x8 -> 512x4x4 (downsample spatially and increase channels)\n",
    "        self.layer4 = ResidualBlock(256, 512, stride=2)\n",
    "\n",
    "        # Average pooling to reduce from 4x4 to 2x2\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2)\n",
    "        # Fully connected layer mapping to num_classes\n",
    "        self.fc = nn.Linear(512 * 2 * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        x = self.initial(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the 2x2 conv output for the linear classifier\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tChqpHE6oGaS"
   },
   "source": [
    "# Vision Transformer (20 pts)\n",
    "The Vision Transformer (ViT), introduced in 2020 by Dosovitskiy et al., applies the Transformer architectures, originally designed for natural language processing, to visual data.\n",
    "\n",
    "![ViT](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q0tvs1aDxi_7Otm_Zgys1A.png)\n",
    "\n",
    "In this section, you are tasked with implementing a Vision Transformer model for the CIFAR-10 or CIFAR-100 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2GWkLNaoGaS"
   },
   "source": [
    "## Implement Patch Embedding Block with Positional Encoding (10 pts)\n",
    "\n",
    "In Vision Transformers (ViTs), the Patch Embedding Block converts input images into a sequence of patch embeddings, enabling the model to process image data using transformer architectures. Since transformers are not inherently aware of the spatial relationships between patches, positional encoding is added to provide this information.\n",
    "\n",
    "Overview:\n",
    "- **Input:** 3x32x32 images, **Arguments:**: `patch_size` (make sure `32 % patch_size == 0` ), `embed_dim`\n",
    "- Divide the image into non-overlapping patches of size `3 x patch_size x patch_size`. You should end up getting `(32 // patch_size)**2` patches.\n",
    "- Flatten the pixels in each patch (into a single dimension of size `3 x patch_size x patch_size`), apply Layer Normalization, project it into a higher-dimensional space (e.g., 256 dimensions) using a fully-connected layer, and then apply another Layer Normalization.\n",
    "- Add positional encodings to the patch embeddings to retain spatial information.\n",
    "\n",
    "You are asked to implement the Patch Embedding Block as follow:\n",
    "- Transform \"b c (h x p) (w x p) -> b (h x w) (p x p x c)\" where b is batch size, c is number of channels, h x p = 32 is the input image height, w x p = 32 is the input image width, and p is the patch size (e.g., 4).\n",
    "- \"b (h x w) (p x p x c)\" -> LayerNorm -> fully-connected layer -> LayerNorm\n",
    "- add positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i-Z7orYPoGaS"
   },
   "outputs": [],
   "source": [
    "def get_positional_encoding(seq_len, embed_dim):\n",
    "    # refer to this paper https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "    # follow the original implementation proposed in Section 3.5\n",
    "    pe = torch.zeros(seq_len, embed_dim)\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, embed_dim, 2):\n",
    "            # TODO\n",
    "            div_term = 10000 ** (i / embed_dim)\n",
    "            pe[pos, i] = math.sin(pos / div_term)\n",
    "            if i + 1 < embed_dim:\n",
    "                pe[pos, i + 1] = math.cos(pos / div_term)\n",
    "    return pe\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        # TODO\n",
    "        self.patch_size = patch_size # Size of each patch (must divide 32)\n",
    "        self.embed_dim = embed_dim # Dimension of the output patch embedding\n",
    "\n",
    "        # Calculate the number of patches. For a 32x32 image\n",
    "        self.num_patches = (32 // patch_size) ** 2\n",
    "        self.patch_dim = 3 * patch_size * patch_size  # since input has 3 channels\n",
    "\n",
    "        # Two LayerNorms sandwiching the linear projection\n",
    "        self.layernorm1 = nn.LayerNorm(self.patch_dim)\n",
    "        self.proj = nn.Linear(self.patch_dim, embed_dim) # Project into higher-dimensional space\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Compute and register positional encodings (non-learnable)\n",
    "        pos_encoding = get_positional_encoding(self.num_patches, embed_dim)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)  # shape: [num_patches, embed_dim]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "    # B = batch size; C = num channels; P = patch size; H x P = 32 is input img height; W x P = 32 is input img width\n",
    "        # b c (h x p) (w x p) -> b (h x w) (p x p x c)\n",
    "        B, C, H, W = x.shape\n",
    "        # Unfold to extract non-overlapping patches\n",
    "        unfold = nn.Unfold(kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        patches = unfold(x)  # shape: [B, patch_dim, num_patches]\n",
    "        patches = patches.transpose(1, 2)  # shape: [B, num_patches, patch_dim]\n",
    "\n",
    "        # Apply first LayerNorm, project, and apply second LayerNorm.\n",
    "        patches = self.layernorm1(patches)\n",
    "        embeddings = self.proj(patches)  # shape: [B, num_patches, embed_dim]\n",
    "        embeddings = self.layernorm2(embeddings)\n",
    "\n",
    "        # Add the positional encoding (broadcasted along the batch dimension)\n",
    "        embeddings = embeddings + self.pos_encoding\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ1QRGnQoGaS"
   },
   "source": [
    "## Implement Vision Transformer (ViT) (10 pts)\n",
    "\n",
    "The Vision Transformer (ViT) model comprises three components (refer to figure above):\n",
    "\n",
    "1. **Patch Embedding:** Converts input images into a sequence of patch embeddings.\n",
    "\n",
    "2. **Transformer Encoder:** Processes the sequence of patch embeddings to capture complex patterns and relationships.\n",
    "\n",
    "3. **MLP Head:** Maps the output from the Transformer Encoder to class predictions.\n",
    "\n",
    "**Implementation Requirements:**\n",
    "\n",
    "- **Input and Output Dimensions:** The model should accept inputs of size 32x32x3 and output a vector with a dimension equal to the number of classes (10 for CIFAR-10 and 100 for CIFAR-100).\n",
    "\n",
    "- **Patch Embedding:** Begin with the PatchEmbedding module you previously implemented.\n",
    "\n",
    "- **Transformer Encoder:** Utilize `nn.TransformerEncoder()` to process the sequence of patch embeddings, capturing high-level representations.\n",
    "\n",
    "- **MLP Head:** Conclude with a mean pooling operation over the temporal dimension (dimension 1), followed by a Multi-Layer Perceptron (MLP) head that maps the pooled embeddings to class predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HREfB_2VoGaT"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=512, depth=6, num_heads=8, num_classes=10): # depth = # of transformer encoder layers; num_heads = num attention heads\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        # TODO\n",
    "        # Patch embedding block with positional encoding\n",
    "        self.patch_embed = PatchEmbedding(patch_size, embed_dim)\n",
    "\n",
    "        # Transformer encoder to capture high-level representations\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        # MLP head: average pooling over patches (happens in fwd) then fully connected layer.\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        # Obtain patch embeddings: [B, num_patches, embed_dim]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Transformer encoder expects shape: [seq_len, B, embed_dim]\n",
    "        x = x.transpose(0, 1)  # now shape: [num_patches, B, embed_dim]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.transpose(0, 1)  # back to shape: [B, num_patches, embed_dim]\n",
    "\n",
    "        # Mean pooling over the patch dimension\n",
    "        x = x.mean(dim=1)  # shape: [B, embed_dim]\n",
    "\n",
    "        # MLP head for class predictions\n",
    "        x = self.mlp_head(x)  # shape: [B, num_classes]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urxGfv7h7YPG"
   },
   "source": [
    "# Training Neural Networks (20 pts)\n",
    "In this section, you will implement a `Trainer` class, use it to train the models that you defined previously, and evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9anAco48Aht"
   },
   "source": [
    "## Check CUDA and GPUs\n",
    "The following code helps you check if CUDA is available and lists the available GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYlsO8GA90vz",
    "outputId": "69e108f5-ce0f-4ee8-fb75-3f11b2bc3120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 2070 with Max-Q Design\n",
      "Current GPU device: 0 - NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "    # Get the name of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "\n",
    "    # Set the current GPU device\n",
    "    device = torch.cuda.current_device()\n",
    "    print(f\"Current GPU device: {device} - {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW7k1eFy_c1U"
   },
   "source": [
    "## Complete the Trainer Class (15 pts)\n",
    "Fill-in all the TODOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jD3MKbt9vn1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, dataset, net, optimizer, loss_function=nn.CrossEntropyLoss(),\n",
    "                 device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.dataset = dataset\n",
    "        self.net = net.to(device)\n",
    "        self.lossFunction = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        # TODO (5 pts): complete training loop\n",
    "        self.net.train()\n",
    "        total_loss = 0.0\n",
    "        for x, y in self.dataset.train_dataloader:\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.net(x)\n",
    "            loss = self.lossFunction(outputs, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(self.dataset.train_dataloader)\n",
    "        return avg_loss\n",
    "\n",
    "    def compute_test_accuracy(self, path):\n",
    "        # TODO (5 pts): compute classification accuracy based on test data\n",
    "        self.net.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in self.dataset.test_dataloader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                outputs = self.net(x)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "        accuracy = 100.0 * correct / total\n",
    "        return accuracy\n",
    "\n",
    "    def train(self, path, num_epochs=20):\n",
    "        self.net.train()  # Set model to training mode\n",
    "        best_accuracy = 0.0\n",
    "        for epoch in range(num_epochs):\n",
    "            # TODO (5 pts): print loss for every epoch, print test accuracy for every 5 epochs\n",
    "            # Feel free to record the training process for analysis\n",
    "            avg_loss = self.train_one_epoch()\n",
    "            # Print training loss every epoch; print test accuracy every 5 epochs\n",
    "            if (epoch + 1) % 5 == 0 or (epoch + 1) == num_epochs:\n",
    "                test_acc = self.compute_test_accuracy(path)\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "                # Save the model if test accuracy improves\n",
    "                if test_acc > best_accuracy:\n",
    "                    best_accuracy = test_acc\n",
    "                    torch.save(self.net.state_dict(), path)\n",
    "                    print(\"Saved best model checkpoint.\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj6DxGwvALFu"
   },
   "source": [
    "## Training (5 pts)\n",
    "Follow these steps to train and evaluate your models (AlexNet, ResNet, and ViT):\n",
    "* Create the model, the dataset, and the optimizer. We suggest using SGD with a learning rate of `1e-2`, but you are welcome to explore other options.\n",
    "* Configure the trainer.\n",
    "* Compute and print test accuracy before training.\n",
    "* Train the model.\n",
    "* Compute and print test accuracy after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLSfjg89955p",
    "outputId": "f6032c5d-7c38-4094-88e8-64a79c5ba0e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 35855178\n",
      "AlexNet Initial Test Accuracy: 10.00%\n",
      "Epoch 1/20 - Loss: 2.0498\n",
      "Epoch 2/20 - Loss: 1.4887\n",
      "Epoch 3/20 - Loss: 1.2185\n",
      "Epoch 4/20 - Loss: 0.9925\n",
      "Epoch 5/20 - Loss: 0.8320, Test Accuracy: 71.00%\n",
      "Saved best model checkpoint.\n",
      "Epoch 6/20 - Loss: 0.7032\n",
      "Epoch 7/20 - Loss: 0.5953\n",
      "Epoch 8/20 - Loss: 0.4998\n",
      "Epoch 9/20 - Loss: 0.4090\n",
      "Epoch 10/20 - Loss: 0.3213, Test Accuracy: 79.24%\n",
      "Saved best model checkpoint.\n",
      "Epoch 11/20 - Loss: 0.2427\n",
      "Epoch 12/20 - Loss: 0.1700\n",
      "Epoch 13/20 - Loss: 0.1213\n",
      "Epoch 14/20 - Loss: 0.0808\n",
      "Epoch 15/20 - Loss: 0.0650, Test Accuracy: 80.79%\n",
      "Saved best model checkpoint.\n",
      "Epoch 16/20 - Loss: 0.0464\n",
      "Epoch 17/20 - Loss: 0.0388\n",
      "Epoch 18/20 - Loss: 0.0301\n",
      "Epoch 19/20 - Loss: 0.0358\n",
      "Epoch 20/20 - Loss: 0.0225, Test Accuracy: 79.99%\n",
      "AlexNet Final Test Accuracy: 79.99%\n",
      "Number of parameters: 5771338\n",
      "ResNet Initial Test Accuracy: 7.38%\n",
      "Epoch 1/20 - Loss: 1.2467\n",
      "Epoch 2/20 - Loss: 0.7317\n",
      "Epoch 3/20 - Loss: 0.4917\n",
      "Epoch 4/20 - Loss: 0.3145\n",
      "Epoch 5/20 - Loss: 0.1700, Test Accuracy: 77.15%\n",
      "Saved best model checkpoint.\n",
      "Epoch 6/20 - Loss: 0.0813\n",
      "Epoch 7/20 - Loss: 0.0304\n",
      "Epoch 8/20 - Loss: 0.0060\n",
      "Epoch 9/20 - Loss: 0.0017\n",
      "Epoch 10/20 - Loss: 0.0010, Test Accuracy: 82.75%\n",
      "Saved best model checkpoint.\n",
      "Epoch 11/20 - Loss: 0.0008\n",
      "Epoch 12/20 - Loss: 0.0006\n",
      "Epoch 13/20 - Loss: 0.0005\n",
      "Epoch 14/20 - Loss: 0.0005\n",
      "Epoch 15/20 - Loss: 0.0004, Test Accuracy: 82.78%\n",
      "Saved best model checkpoint.\n",
      "Epoch 16/20 - Loss: 0.0004\n",
      "Epoch 17/20 - Loss: 0.0003\n",
      "Epoch 18/20 - Loss: 0.0003\n",
      "Epoch 19/20 - Loss: 0.0003\n",
      "Epoch 20/20 - Loss: 0.0003, Test Accuracy: 82.93%\n",
      "Saved best model checkpoint.\n",
      "ResNet Final Test Accuracy: 82.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spenc\\OneDrive\\Desktop\\csci677\\677-3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 18946666\n",
      "ViT Initial Test Accuracy: 10.90%\n",
      "Epoch 1/20 - Loss: 1.8558\n",
      "Epoch 2/20 - Loss: 1.4660\n",
      "Epoch 3/20 - Loss: 1.2994\n",
      "Epoch 4/20 - Loss: 1.1695\n",
      "Epoch 5/20 - Loss: 1.0827, Test Accuracy: 58.01%\n",
      "Saved best model checkpoint.\n",
      "Epoch 6/20 - Loss: 1.0209\n",
      "Epoch 7/20 - Loss: 0.9421\n",
      "Epoch 8/20 - Loss: 0.8788\n",
      "Epoch 9/20 - Loss: 0.8265\n",
      "Epoch 10/20 - Loss: 0.7617, Test Accuracy: 64.66%\n",
      "Saved best model checkpoint.\n",
      "Epoch 11/20 - Loss: 0.7089\n",
      "Epoch 12/20 - Loss: 0.6598\n",
      "Epoch 13/20 - Loss: 0.6002\n",
      "Epoch 14/20 - Loss: 0.5535\n",
      "Epoch 15/20 - Loss: 0.5092, Test Accuracy: 65.68%\n",
      "Saved best model checkpoint.\n",
      "Epoch 16/20 - Loss: 0.4667\n",
      "Epoch 17/20 - Loss: 0.4236\n",
      "Epoch 18/20 - Loss: 0.3884\n",
      "Epoch 19/20 - Loss: 0.3525\n",
      "Epoch 20/20 - Loss: 0.3193, Test Accuracy: 64.70%\n",
      "ViT Final Test Accuracy: 64.70%\n"
     ]
    }
   ],
   "source": [
    "# AlexNet train and evaluation\n",
    "# TODO\n",
    "dataset = CIFAR10Dataset(batch_size=128)\n",
    "alexnet_model = AlexNet(num_classes=10)\n",
    "print(f\"Number of parameters: {count_parameters(alexnet_model)}\")\n",
    "optimizer_alex = optim.SGD(alexnet_model.parameters(), lr=1e-2, momentum=0.9)\n",
    "trainer_alexnet = Trainer(dataset, alexnet_model, optimizer_alex)\n",
    "\n",
    "initial_acc = trainer_alexnet.compute_test_accuracy(\"alexnet_best.pth\")\n",
    "print(\"AlexNet Initial Test Accuracy: {:.2f}%\".format(initial_acc))\n",
    "trainer_alexnet.train(\"alexnet_best.pth\", num_epochs=20)\n",
    "final_acc = trainer_alexnet.compute_test_accuracy(\"alexnet_best.pth\")\n",
    "print(\"AlexNet Final Test Accuracy: {:.2f}%\".format(final_acc))\n",
    "\n",
    "# ResNet train and evaluation\n",
    "# TODO\n",
    "dataset = CIFAR10Dataset(batch_size=128)\n",
    "resnet_model = ResNet(num_classes=10)\n",
    "print(f\"Number of parameters: {count_parameters(resnet_model)}\")\n",
    "optimizer_resnet = optim.SGD(resnet_model.parameters(), lr=1e-2, momentum=0.9)\n",
    "trainer_resnet = Trainer(dataset, resnet_model, optimizer_resnet)\n",
    "\n",
    "initial_acc = trainer_resnet.compute_test_accuracy(\"resnet_best.pth\")\n",
    "print(\"ResNet Initial Test Accuracy: {:.2f}%\".format(initial_acc))\n",
    "trainer_resnet.train(\"resnet_best.pth\", num_epochs=20)\n",
    "final_acc = trainer_resnet.compute_test_accuracy(\"resnet_best.pth\")\n",
    "print(\"ResNet Final Test Accuracy: {:.2f}%\".format(final_acc))\n",
    "\n",
    "# ViT train and evaluation\n",
    "# TODO\n",
    "dataset = CIFAR10Dataset(batch_size=128)\n",
    "vit_model = VisionTransformer(img_size=32, patch_size=4, in_channels=3, embed_dim=512, depth=6, num_heads=8, num_classes=10)\n",
    "print(f\"Number of parameters: {count_parameters(vit_model)}\")\n",
    "optimizer_vit = optim.SGD(vit_model.parameters(), lr=1e-2, momentum=0.9)\n",
    "trainer_vit = Trainer(dataset, vit_model, optimizer_vit)\n",
    "\n",
    "initial_acc = trainer_vit.compute_test_accuracy(\"vit_best.pth\")\n",
    "print(\"ViT Initial Test Accuracy: {:.2f}%\".format(initial_acc))\n",
    "trainer_vit.train(\"vit_best.pth\", num_epochs=20)\n",
    "final_acc = trainer_vit.compute_test_accuracy(\"vit_best.pth\")\n",
    "print(\"ViT Final Test Accuracy: {:.2f}%\".format(final_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FX2-yUziAi5w"
   },
   "source": [
    "## Evaluation using Confusion Matrix (5 pts)\n",
    "A confusion matrix is a fundamental tool for evaluating the performance of classification models. Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class.\n",
    "\n",
    "You are asked to evaluate your trained model by computing and printing the confusion matrix. You can either compute it by yourself or use sklearn.metrics.confusion_matrix()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "giA5Kg53DBwX",
    "outputId": "2cd34699-0b16-4705-f802-ba8ce1109e7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet Confusion Matrix:\n",
      "Confusion Matrix:\n",
      "[[825  10  48  30  18   4   9   5  28  23]\n",
      " [ 11 889   9   7   3   1   7   1  22  50]\n",
      " [ 33   2 769  52  72  27  30  10   4   1]\n",
      " [ 15   2  69 715  58  69  41  20   5   6]\n",
      " [  8   0  57  50 827  12  22  21   3   0]\n",
      " [  5   3  68 249  41 598   9  22   1   4]\n",
      " [  2   2  38  60  32  11 847   3   3   2]\n",
      " [  6   1  40  51  64  27   3 801   1   6]\n",
      " [ 55  17  21  11   9   1   2   3 857  24]\n",
      " [ 18  54  11  14   5   2   7   5  13 871]]\n",
      "ResNet Confusion Matrix:\n",
      "Confusion Matrix:\n",
      "[[847  13  31  13  13   2   7   9  42  23]\n",
      " [  9 914   1   4   3   1   4   1  14  49]\n",
      " [ 52   2 738  38  51  41  49  21   4   4]\n",
      " [ 13   3  56 679  42 123  45  20   8  11]\n",
      " [  9   3  49  46 811  15  29  31   6   1]\n",
      " [ 13   4  33 133  30 747   6  29   2   3]\n",
      " [  3   4  36  39  15  13 880   4   4   2]\n",
      " [ 12   1  16  22  36  36   6 864   1   6]\n",
      " [ 40  11   6   7   0   3   3   4 915  11]\n",
      " [ 18  44   3   9   1   2   3  10  12 898]]\n",
      "ViT Confusion Matrix:\n",
      "Confusion Matrix:\n",
      "[[538  30  79  51  12   6  16  16 210  42]\n",
      " [ 14 734  10  17   8   9   7  11  96  94]\n",
      " [ 46   6 564  93  62  69  75  43  33   9]\n",
      " [  5   6  58 524  33 241  51  38  29  15]\n",
      " [ 16   6 102  93 528  52  63 103  27  10]\n",
      " [  5   2  42 192  35 629  18  57   7  13]\n",
      " [  7   8  54 117  40  35 701  11  18   9]\n",
      " [ 13   5  38  74  61  78   9 687  20  15]\n",
      " [ 24  28  21  20   6   3   8   2 874  14]\n",
      " [ 13 120  13  20   6   9   5  18 105 691]]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def compute_confusion_matrix(trainer):\n",
    "    trainer.net.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in trainer.dataset.test_dataloader:\n",
    "            x = x.to(trainer.device)\n",
    "            outputs = trainer.net(x)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "print(\"AlexNet Confusion Matrix:\")\n",
    "compute_confusion_matrix(trainer_alexnet)\n",
    "\n",
    "print(\"ResNet Confusion Matrix:\")\n",
    "compute_confusion_matrix(trainer_resnet)\n",
    "\n",
    "print(\"ViT Confusion Matrix:\")\n",
    "compute_confusion_matrix(trainer_vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdkvCp5bDVe8"
   },
   "source": [
    "## Observations (15 pts)\n",
    "Write down your observations regarding the results you obtained throughout this assignment. Here are some suggestions:\n",
    "* **Accuracy and Loss Curves**: Plot and compare the training and validation accuracy and loss curves for each model. This helps visualize how well each model is learning over time and whether they are overfitting or underfitting.\n",
    "* **Top Misclassified Images**: Examine the classes that are most frequently misclassified by each model. This can provide insights into the types of images that are challenging for each model and may suggest areas for improvement.\n",
    "* **Feature Visualization**: Visualize the feature maps or activations of intermediate layers in each CNN. This can help you understand what features or patterns each model is learning and whether they differ in terms of learned representations.\n",
    "* **Robustness Testing**: Assess the robustness of each model by introducing noise, transformations, or adversarial examples to the test data. This can help identify which models are more resilient to perturbations.\n",
    "* **Runtime and Resource Usage**: Compare the training time and resource usage (e.g., GPU memory) of each model.\n",
    "* **Hyperparameter Tuning**: Analyze the impact of hyperparameters (learning rates, batch sizes, etc.) on training speed and convergence.\n",
    "* **Model Size and Efficiency**: Analyze the trade-off between model size and accuracy for each model.\n",
    "* **Ablation Studies**: Conduct ablation studies by removing or modifying specific components (e.g., dropout, batch normalization, etc.) of each model to understand their contributions to performance.\n",
    "\n",
    "You don't need to follow them. Feel free to write down any observation you have, or to use tools like Tensorboard to support your observations. You are also welcome to give comments on the design of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIDgYVOrGcZI"
   },
   "source": [
    "## **TODO: write down your observations**\n",
    "\n",
    "These results are fairly in line with my expectations. The CNNs were more or less designed around this classification set and dataset. The confusion matrices are cool to see--I haven't used them before. They show the CNNs, and ResNet in particular, have good numbers on the diagonal showing their good accuracy. When they struggle, it is usually involving just one other class, which likely indicated some crossover or confusion between objects with similar visual features (ie birds/planes or dogs/cats). The ViT confusion matrix is a bit all over the place. The main things holding back the ViT are likely 1) not enough data to train on, and 2) only running 20 epochs. The CIFAR datasets only have 60,000 images each. For the ViT to preform well, millions of images would be desirable. There may be possible slight gains from hyperparameter tuning, but the real key here is the data for transformers. The local connectivity and shared weights architecture of the CNNs give them an edge on classification tasks with the limited data. \n",
    "\n",
    "The ViT was by far the most challenging to implement. Particularly figuring out the math behind the Patch Embedding. Conceptually it made sense to me, but getting it to function as desired was definitely a challenge. It was helpful to have the paper cited for reference. It would have been cool to see the ViT perform well (have a part of the assignment use a large dataset), but I know that kind of implementation can be implausible given compute contraints of students. I may extend this work to visualize the patch embedings for my own learning since that was the section I struggled with most.\n",
    "\n",
    "Thanks for putting together a solid assignment! "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "677-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
