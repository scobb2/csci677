{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u93YJyjNB9WK"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this assignment you will practice putting together a simple image classification pipeline with both non-parametric and parametric methods.\n",
        "\n",
        "In paticular, we will work with the k-Nearest Neighbor, the SVM classifier and the 2-Layered Neural Network for [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. The goals of this assignment are as follows:\n",
        "\n",
        "\n",
        "\n",
        "*   Understand the basic Image Classification pipeline and the data-driven approach (train/predict stages).\n",
        "*   Understand the train/val/test splits and the use of validation data for hyperparameter tuning.\n",
        "*   Implement and apply a Weighted k-Nearest Neighbor (kNN) classifier.\n",
        "*   Implement and apply a Multiclass Support Vector Machine (SVM) classifier.\n",
        "*   Implement and apply a 2-layered Neural Network.\n",
        "*   Understand the differences and tradeoffs between these classifiers.\n",
        "\n",
        "Please fill in all the **TODO** code blocks. Once you are ready to submit:\n",
        "\n",
        "* Export the notebook `CSCI677_spring25_assignment_2.ipynb` as a PDF `[Your USC ID]_CSCI677_spring25_assignment_2.pdf`\n",
        "* Submit your PDF file through Brightspace.\n",
        "\n",
        "Please make sure that the notebook have been run before exporting PDF, and your code and all cell outputs are visible in the your submitted PDF. Regrading request will not be accepted if your code/output is not visible in the original submission. Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_7K_0sWz0OZ"
      },
      "source": [
        "# **Data Preparation**\n",
        "\n",
        "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) is a well known dataset composed of 60,000 colored 32x32 images. The utility function `cifar10()` returns the entire CIFAR-10 dataset as a set of four Torch tensors:\n",
        "* `x_train` contains all training images (real numbers in the range  [0,1] )\n",
        "* `y_train` contains all training labels (integers in the range  [0,9] )\n",
        "* `x_test` contains all test images\n",
        "* `y_test` contains all test labels\n",
        "\n",
        "This function automatically downloads the CIFAR-10 dataset the first time you run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR10\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVP8si9RpSdc"
      },
      "outputs": [],
      "source": [
        "def _extract_tensors(dset, num=None):\n",
        "    x = torch.tensor(dset.data, dtype=torch.float32).permute(0, 3, 1, 2).div_(255)\n",
        "    y = torch.tensor(dset.targets, dtype=torch.int64)\n",
        "    if num is not None:\n",
        "        if num <= 0 or num > x.shape[0]:\n",
        "          raise ValueError('Invalid value num=%d; must be in the range [0, %d]'\n",
        "                          % (num, x.shape[0]))\n",
        "        x = x[:num].clone()\n",
        "        y = y[:num].clone()\n",
        "    return x, y\n",
        "\n",
        "def cifar10(num_train=None, num_test=None):\n",
        "    download = not os.path.isdir('cifar-10-batches-py')\n",
        "    dset_train = CIFAR10(root='.', download=download, train=True)\n",
        "    dset_test = CIFAR10(root='.', train=False)\n",
        "    x_train, y_train = _extract_tensors(dset_train, num_train)\n",
        "    x_test, y_test = _extract_tensors(dset_test, num_test)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz09FqISr0c8"
      },
      "source": [
        "Our data is going to be stored simply in the four variables: `x_train`, `x_test`, `y_train`, and `y_test`.\n",
        "\n",
        "\n",
        "*   Training set: `x_train` is composed of 50,000 images where `y_train` references the corresponding labels.\n",
        "*   Testing set: `x_test` is composed of 10,000 images where `y_test` references the corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "num_train = 50000\n",
        "num_test = 5000\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "x_train, y_train, x_test, y_test = cifar10(num_train, num_test)\n",
        "\n",
        "# Split the data into train, val, and test sets. In addition we will\n",
        "# create a small development set as a subset of the training data;\n",
        "# we can use this for development so our code runs faster.\n",
        "num_training = 49000\n",
        "num_validation = 1000\n",
        "num_test = 1000\n",
        "num_dev = 500\n",
        "\n",
        "x_train_np = x_train.numpy()\n",
        "y_train_np = y_train.numpy()\n",
        "x_test_np = x_test.numpy()\n",
        "y_test_np = y_test.numpy()\n",
        "\n",
        "# Our validation set will be num_validation points from the original\n",
        "# training set.\n",
        "mask = range(num_training, num_training + num_validation)\n",
        "X_val = x_train_np[mask]\n",
        "y_val = y_train_np[mask]\n",
        "\n",
        "# Our training set will be the first num_train points from the original\n",
        "# training set.\n",
        "mask = range(num_training)\n",
        "X_train = x_train_np[mask]\n",
        "y_train = y_train_np[mask]\n",
        "\n",
        "# We will also make a development set, which is a small subset of\n",
        "# the training set.\n",
        "mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "X_dev = x_train_np[mask]\n",
        "y_dev = y_train_np[mask]\n",
        "\n",
        "# We use the first num_test points of the original test set as our\n",
        "# test set.\n",
        "mask = range(num_test)\n",
        "X_test = x_test_np[mask]\n",
        "y_test = y_test_np[mask]\n",
        "\n",
        "# Preprocessing: reshape the image data into rows\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
        "\n",
        "# As a sanity check, print out the shapes of the data\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('dev data shape: ', X_dev.shape)\n",
        "\n",
        "# Preprocessing: subtract the mean image\n",
        "# first: compute the image mean based on the training data\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "\n",
        "# second: subtract the mean image from train and test data\n",
        "X_train -= mean_image\n",
        "X_val -= mean_image\n",
        "X_test -= mean_image\n",
        "X_dev -= mean_image\n",
        "\n",
        "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
        "# only has to worry about optimizing a single weight matrix W.\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
        "\n",
        "X_train, X_test, X_dev, X_val = torch.FloatTensor(X_train), torch.FloatTensor(X_test), torch.FloatTensor(X_dev), torch.FloatTensor(X_val)\n",
        "y_train, y_test, y_dev, y_val = torch.LongTensor(y_train), torch.LongTensor(y_test), torch.LongTensor(y_dev), torch.LongTensor(y_val)\n",
        "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr_Wv6Hi3Y_K"
      },
      "source": [
        "# k-Nearest Neighbor (kNN) (20 pts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp9dU9BxtSsd"
      },
      "source": [
        "## **Subsampling**\n",
        "\n",
        "When implementing machine learning algorithms, it's usually a good idea to use a small sample of the full dataset. This way your code will run much faster, allowing for more interactive and efficient development. Once you are satisfied that you have correctly implemented the algorithm, you can then rerun with the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HnkODySthr0"
      },
      "outputs": [],
      "source": [
        "# Subsample size\n",
        "def subsample(X, y, n):\n",
        "    assert len(X) == len(y)\n",
        "    indices = torch.randint(len(X), (n,))\n",
        "    return X[indices],  y[indices]\n",
        "ss_x_train, ss_y_train = subsample(X_train, y_train, 500)\n",
        "print(ss_x_train.shape, ss_y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hPrEvn9FSPf"
      },
      "source": [
        "## Compute Distance (5 pts)\n",
        "\n",
        "Now that we have examined and prepared our data, it is time to implement the Weighted-kNN classifier. We can break the process down into two steps:\n",
        "1. Compute the consine similarities between all training examples and all test examples\n",
        "2. Given these pre-computed similarities, for each test example find its k nearest neighbors and have them vote for the label to output\n",
        "\n",
        "**NOTE**: When implementing algorithms in PyTorch, it's best to avoid loops in Python if possible. Instead it is preferable to implement your computation so that all loops happen inside PyTorch functions. This will usually be much faster than writing your own loops in Python, since PyTorch functions can be internally optimized to iterate efficiently, possibly using multiple threads. This is especially important when using a GPU to accelerate your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxg2Aq1pt9fy"
      },
      "outputs": [],
      "source": [
        "def compute_distances(x_train, x_test):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    x_train: shape (num_train, C, H, W) tensor.\n",
        "    x_test: shape (num_test, C, H, W) tensor.\n",
        "\n",
        "    Returns:\n",
        "    dists: shape (num_train, num_test) tensor where dists[j, i] is the\n",
        "        cosine similarity between the ith training image and the jth test\n",
        "        image.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the number of training and testing images\n",
        "    num_train = x_train.shape[0]\n",
        "    num_test = x_test.shape[0]\n",
        "\n",
        "    # dists will be the tensor housing all distance measurements between testing and training\n",
        "    dists = x_train.new_zeros(num_train, num_test)\n",
        "\n",
        "    # Flatten tensors\n",
        "    train = x_train.flatten(1)\n",
        "    test = x_test.flatten(1)\n",
        "\n",
        "    #######################################################################\n",
        "    # TODO (5 pts):\n",
        "    # find the consine similarities between testing and training images,\n",
        "    # and save the computed distance in dists.\n",
        "    #######################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    \n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    return dists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1i6QyD4j6bB"
      },
      "source": [
        "## Implement Weighted-kNN (10 pts)\n",
        "\n",
        "The Weighted-kNN classifier consists of two stages:\n",
        "\n",
        "*   Training: the classifier takes the training data and simply remembers it\n",
        "*   Testing: For each test sample, the classifier computes the similarity to all training samples and selects the k most similar neighbors. Instead of simple majority voting, each neighbor contributes to the final prediction based on its similarity with the test sample. This ensures that more similar neighbors have a greater influence on the classification decision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrYRDVcuuL8m"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class KnnClassifier:\n",
        "    def __init__(self, x_train, y_train):\n",
        "        \"\"\"\n",
        "        x_train: shape (num_train, C, H, W) tensor where num_train is batch size,\n",
        "          C is channel size, H is height, and W is width.\n",
        "        y_train: shape (num_train) tensor where num_train is batch size providing labels\n",
        "        \"\"\"\n",
        "\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def predict(self, x_test, k=1):\n",
        "        \"\"\"\n",
        "        x_test: shape (num_test, C, H, W) tensor where num_test is batch size,\n",
        "          C is channel size, H is height, and W is width.\n",
        "        k: The number of neighbors to use for prediction\n",
        "        \"\"\"\n",
        "\n",
        "        # Init output shape\n",
        "        y_test_pred = torch.zeros(x_test.shape[0], dtype=torch.int64)\n",
        "\n",
        "        # Find & store Euclidean distance between test & train\n",
        "        dists = compute_distances(self.x_train, x_test)\n",
        "\n",
        "        #######################################################################\n",
        "        # TODO (10 pts):\n",
        "        # The goal is to return a tensor y_test_pred where the ith index\n",
        "        # is the assigned label to ith test image by the kNN algorithm.\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1. Index over test images\n",
        "\n",
        "        # 2. Find the indices of the k most similar training samples (highest cosine similarity).\n",
        "\n",
        "        # 3. Retrieve the labels of these k neighbors and compute their contributions as the similarity scores\n",
        "\n",
        "        # 4. Assign the label with the highest accumulated weight as the final prediction.\n",
        "        #######################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        \n",
        "        return y_test_pred\n",
        "\n",
        "    def check_accuracy(self, x_test, y_test, k=1, quiet=False):\n",
        "        \"\"\"\n",
        "        x_test: shape (num_test, C, H, W) tensor where num_test is batch size,\n",
        "          C is channel size, H is height, and W is width.\n",
        "        y_test: shape (num_test) tensor where num_test is batch size providing labels\n",
        "        k: The number of neighbors to use for prediction\n",
        "        quiet: If True, don't print a message.\n",
        "\n",
        "        Returns:\n",
        "        accuracy: Accuracy of this classifier on the test data, as a percent.\n",
        "          Python float in the range [0, 100]\n",
        "        \"\"\"\n",
        "\n",
        "        y_test_pred = self.predict(x_test, k=k)\n",
        "        num_samples = x_test.shape[0]\n",
        "        num_correct = (y_test == y_test_pred).sum().item()\n",
        "        accuracy = 100.0 * num_correct / num_samples\n",
        "        msg = (f'Got {num_correct} / {num_samples} correct; '\n",
        "              f'accuracy is {accuracy:.2f}%')\n",
        "        if not quiet:\n",
        "          print(msg)\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpaBIPfczlo5"
      },
      "source": [
        "We've finished implementing kNN and can begin testing the algorithm on larger portions of the dataset to see how well it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXqBT2ZnzDFq"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "num_train = 5000\n",
        "num_test = 500\n",
        "num_val = 500\n",
        "knn_x_train, knn_y_train = subsample(X_train, y_train, num_train)\n",
        "knn_x_test, knn_y_test = subsample(X_test, y_test, num_test)\n",
        "knn_x_val, knn_y_val = subsample(X_val, y_val, num_val)\n",
        "classifier = KnnClassifier(knn_x_train, knn_y_train)\n",
        "classifier.check_accuracy(knn_x_test, knn_y_test, k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Tuning (5 pts)\n",
        "\n",
        "Now we use the validation set to tune hyperparameters (number of nearest neighbors k). You should experiment with different ranges of k."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}\n",
        "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
        "best_k = None # The value of k that achieved the highest validation rate.\n",
        "\n",
        "################################################################################\n",
        "# TODO (5 pts):                                                               #\n",
        "# Write code that chooses the best k value by tuning on the validation         #\n",
        "# set. For each value of k, train a KnnClassifier on the                       #\n",
        "# training set, compute its accuracy on the training and validation sets, and  #\n",
        "# store these numbers in the results dictionary. In addition, store the best   #\n",
        "# validation accuracy in best_val and the best value of k in best_k.           #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# fill in your own values\n",
        "k_choices = []\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# Print out results.\n",
        "for k in sorted(results):\n",
        "    val_accuracy = results[k]\n",
        "    print('k %d val accuracy: %f' % (\n",
        "                k, val_accuracy))\n",
        "\n",
        "print('best validation accuracy achieved: %f' % best_val)\n",
        "\n",
        "classifier = KnnClassifier(knn_x_train, knn_y_train)\n",
        "test_acc = classifier.check_accuracy(knn_x_test, knn_y_test, k=best_k)\n",
        "print('final test accuracy knn achieved: %f' % test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Pr5Oh5rlUq"
      },
      "source": [
        "# Define a General Classifier Class (15 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fZSgxj-s_jf"
      },
      "source": [
        "Before implementing Support Vector Machine (SVM) Classifier. We define a general classifier class that contains the following main functions:\n",
        "\n",
        "\n",
        "1.   `train`: train this linear classifier using stochastic gradient descent.\n",
        "2.   `predict`: use the trained weights of this linear classifier to predict labels for data points.\n",
        "3.   `loss`: compute the loss function and its derivative.\n",
        "\n",
        "We will define SVM and Softmax classifier as subclasses of this general linear classifier class. Subclasses will override the `loss` function.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0Id2-q_QF02"
      },
      "outputs": [],
      "source": [
        "class LinearClassifier(object):\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        X,\n",
        "        y,\n",
        "        learning_rate=1e-3,\n",
        "        reg=1e-5,\n",
        "        num_iters=100,\n",
        "        batch_size=200,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "          means that X[i] has label 0 <= c < C for C classes.\n",
        "        - learning_rate: (float) learning rate for optimization.\n",
        "        - reg: (float) regularization strength.\n",
        "        - num_iters: (integer) number of steps to take when optimizing\n",
        "        - batch_size: (integer) number of training examples to use at each step.\n",
        "        - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "        num_train, dim = X.shape\n",
        "        num_classes = (\n",
        "            np.max(y) + 1\n",
        "        )  # assume y takes values 0...K-1 where K is number of classes\n",
        "        if self.W is None:\n",
        "            # lazily initialize W\n",
        "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO (5 pts):                                                        #\n",
        "            # Sample batch_size elements from the training data and their           #\n",
        "            # corresponding labels to use in this round of gradient descent.        #\n",
        "            # Store the data in X_batch and their corresponding labels in           #\n",
        "            # y_batch; after sampling X_batch should have shape (batch_size, dim)   #\n",
        "            # and y_batch should have shape (batch_size,)                           #\n",
        "            #########################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "      \n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # perform parameter update\n",
        "            #########################################################################\n",
        "            # TODO (5 pts):                                                         #\n",
        "            # Update the weights using the gradient and the learning rate.          #\n",
        "            #########################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "          array of length N, and each element is an integer giving the predicted\n",
        "          class.\n",
        "        \"\"\"\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        ###########################################################################\n",
        "        # TODO (5 pts):                                                           #\n",
        "        # Implement this method. Store the predicted labels in y_pred.            #\n",
        "        ###########################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        \n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return y_pred\n",
        "\n",
        "    def loss(self, X_batch, y_batch, reg):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative.\n",
        "        Subclasses will override this.\n",
        "\n",
        "        Inputs:\n",
        "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "          data points; each point has dimension D.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "        - reg: (float) regularization strength.\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to self.W; an array of the same shape as W\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcO2mGt3NQ9n"
      },
      "source": [
        "# Multiclass Support Vector Machine (SVM) (25 pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3_Z4W_ZH59m"
      },
      "source": [
        "[Support vector machines (SVMs)](https://scikit-learn.org/stable/modules/svm.html) are a set of supervised learning methods used for classification.\n",
        "\n",
        "The advantages of support vector machines are:\n",
        "\n",
        "* Effective in high dimensional spaces.\n",
        "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
        "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
        "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
        "\n",
        "The disadvantages of support vector machines include:\n",
        "\n",
        "* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
        "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
        "\n",
        "In this section, we will first implement the loss function for SVM and use the validation set to tune hyperparameters.\n",
        "\n",
        "**NOTE:** please use [numpy](https://numpy.org/), please do not use [scikit-learn](https://scikit-learn.org/stable/), [PyTorch](https://pytorch.org/) or other libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2ajMlXEIjWz"
      },
      "source": [
        "## Loss Function (20 pts)\n",
        "\n",
        "We first structure the loss function for SVM. For detailed explanations of SVM loss, please check out [this reading material](https://cs231n.github.io/linear-classify/#loss-function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edfvcABEN_np"
      },
      "outputs": [],
      "source": [
        "def svm_loss(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Structured SVM loss function implementation.\n",
        "\n",
        "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
        "    of N examples.\n",
        "\n",
        "    Inputs:\n",
        "    - W: A numpy array of shape (D, C) containing weights.\n",
        "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "      that X[i] has label c, where 0 <= c < C.\n",
        "    - reg: (float) regularization strength\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W; an array of same shape as W\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "    dW = np.zeros(W.shape)  # initialize the gradient as zero\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO (10 pts):                                                            #\n",
        "    # Implement a vectorized version of the structured SVM loss, storing the    #\n",
        "    # result in loss. Refer to https://cs231n.github.io/linear-classify/        #\n",
        "    #############################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    \n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO (10 pts):                                                            #\n",
        "    # Implement a vectorized version of the gradient for the structured SVM     #\n",
        "    # loss, storing the result in dW.                                           #\n",
        "    #                                                                           #\n",
        "    # Hint: Instead of computing the gradient from scratch, it may be easier    #\n",
        "    # to reuse some of the intermediate values that you used to compute the     #\n",
        "    # loss.                                                                     #\n",
        "    #############################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    \n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    return loss, dW\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlw6rH_mQwAa"
      },
      "source": [
        "Now, we can test our implementation of SVM loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu2PYRB2QbPC"
      },
      "outputs": [],
      "source": [
        "# generate a random SVM weight matrix of small numbers\n",
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "\n",
        "tic = time.time()\n",
        "loss, _ = svm_loss(W, X_dev.numpy(), y_dev.numpy(), 0.000005)\n",
        "toc = time.time()\n",
        "print('loss: %e computed in %fs' % (loss, toc - tic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWpju06hQIYX"
      },
      "outputs": [],
      "source": [
        "class LinearSVM(LinearClassifier):\n",
        "    \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
        "\n",
        "    def loss(self, X_batch, y_batch, reg):\n",
        "        return svm_loss(self.W, X_batch, y_batch, reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFopfak1QPI6"
      },
      "outputs": [],
      "source": [
        "svm = LinearSVM()\n",
        "tic = time.time()\n",
        "loss_hist = svm.train(X_train.numpy(), y_train.numpy(), learning_rate=1e-7, reg=2.5e4,\n",
        "                      num_iters=1500, verbose=True)\n",
        "toc = time.time()\n",
        "print('That took %fs' % (toc - tic))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwJY1JLmQeBQ"
      },
      "outputs": [],
      "source": [
        "y_train_pred = svm.predict(X_train.numpy())\n",
        "print('training accuracy: %f' % (np.mean(y_train.numpy() == y_train_pred), ))\n",
        "y_val_pred = svm.predict(X_val.numpy())\n",
        "print('validation accuracy: %f' % (np.mean(y_val.numpy() == y_val_pred), ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlA3kc3BIr2V"
      },
      "source": [
        "## Hyperparameter Tuning (5 pts)\n",
        "\n",
        "Now we use the validation set to tune hyperparameters (regularization strength and learning rate). You should experiment with different ranges for the learning rates and regularization strengths.\n",
        "\n",
        "**Note:** you may see runtime/overflow warnings during hyper-parameter search. This may be caused by extreme values, and is not a bug."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GII3_6w-QetA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# results is dictionary mapping tuples of the form\n",
        "# (learning_rate, regularization_strength) to tuples of the form\n",
        "# (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n",
        "# of data points that are correctly classified.\n",
        "results = {}\n",
        "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
        "best_svm = None # The LinearSVM object that achieved the highest validation rate.\n",
        "\n",
        "################################################################################\n",
        "# TODO (10 pts):                                                               #\n",
        "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
        "# set. For each combination of hyperparameters, train a linear SVM on the      #\n",
        "# training set, compute its accuracy on the training and validation sets, and  #\n",
        "# store these numbers in the results dictionary. In addition, store the best   #\n",
        "# validation accuracy in best_val and the LinearSVM object that achieves this  #\n",
        "# accuracy in best_svm.                                                        #\n",
        "#                                                                              #\n",
        "# Hint: You should use a small value for num_iters as you develop your         #\n",
        "# validation code so that the SVMs don't take much time to train; once you are #\n",
        "# confident that your validation code works, you should rerun the validation   #\n",
        "# code with a larger value for num_iters.                                      #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# Fill in your own values\n",
        "learning_rates = []\n",
        "regularization_strengths = []\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# Print out results.\n",
        "for lr, reg in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
        "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, train_accuracy, val_accuracy))\n",
        "\n",
        "print('best validation accuracy achieved: %f' % best_val)\n",
        "y_test_pred = best_svm.predict(X_test.numpy())\n",
        "test_acc = np.mean(y_test.numpy() == y_test_pred)\n",
        "print('final test accuracy svm achieved: %f' % test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing a Neural Network (40 pts)\n",
        "In this exercise we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a cross-entropy loss function and L2 regularization on the weight matrices. The network uses a Sigmoid nonlinearity after the first fully connected layer. \n",
        "\n",
        "In other words, the network has the following architecture:\n",
        "\n",
        "  input -> fully connected layer -> Sigmoid -> fully connected layer -> softmax -> cross-entropy\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "**Note**: When you implement the regularization over W, **please DO NOT multiply the regularization term by 1/2** (no coefficient). \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Template class modules that we will use later: Do not edit/modify this class\n",
        "class TwoLayerNet(object):\n",
        "  def __init__(self, input_size, hidden_size, output_size,\n",
        "               dtype=torch.float32, device='cuda', std=1e-4):\n",
        "    \"\"\"\n",
        "    Initialize the model. Weights are initialized to small random values and\n",
        "    biases are initialized to zero. Weights and biases are stored in the\n",
        "    variable self.params, which is a dictionary with the following keys:\n",
        "\n",
        "    W1: First layer weights; has shape (D, H)\n",
        "    b1: First layer biases; has shape (H,)\n",
        "    W2: Second layer weights; has shape (H, C)\n",
        "    b2: Second layer biases; has shape (C,)\n",
        "\n",
        "    Inputs:\n",
        "    - input_size: The dimension D of the input data.\n",
        "    - hidden_size: The number of neurons H in the hidden layer.\n",
        "    - output_size: The number of classes C.\n",
        "    - dtype: Optional, data type of each initial weight params\n",
        "    - device: Optional, whether the weight params is on GPU or CPU\n",
        "    - std: Optional, initial weight scaler.\n",
        "    \"\"\"\n",
        "    # reset seed before start\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    self.params = {}\n",
        "    self.params['W1'] = std * torch.randn(input_size, hidden_size, dtype=dtype, device=device)\n",
        "    self.params['b1'] = torch.zeros(hidden_size, dtype=dtype, device=device)\n",
        "    self.params['W2'] = std * torch.randn(hidden_size, output_size, dtype=dtype, device=device)\n",
        "    self.params['b2'] = torch.zeros(output_size, dtype=dtype, device=device)\n",
        "\n",
        "  def loss(self, X, y=None, reg=0.0):\n",
        "    return nn_forward_backward(self.params, X, y, reg)\n",
        "\n",
        "  def train(self, X, y, X_val, y_val,\n",
        "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
        "            reg=5e-6, num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "    return nn_train(\n",
        "            self.params,\n",
        "            nn_forward_backward,\n",
        "            nn_predict,\n",
        "            X, y, X_val, y_val,\n",
        "            learning_rate, learning_rate_decay,\n",
        "            reg, num_iters, batch_size, verbose)\n",
        "\n",
        "  def predict(self, X):\n",
        "    return nn_predict(self.params, nn_forward_backward, X)\n",
        "\n",
        "  def save(self, path):\n",
        "    torch.save(self.params, path)\n",
        "    print(\"Saved in {}\".format(path))\n",
        "\n",
        "  def load(self, path):\n",
        "    checkpoint = torch.load(path, map_location='cpu')\n",
        "    self.params = checkpoint\n",
        "    print(\"load checkpoint file: {}\".format(path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Forward pass function (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nn_forward_pass(params, X):\n",
        "    \"\"\"\n",
        "    The first stage of our neural network implementation: Run the forward pass\n",
        "    of the network to compute the hidden layer features and classification\n",
        "    scores. The network architecture should be:\n",
        "\n",
        "    FC layer -> ReLU (hidden) -> FC layer (scores)\n",
        "\n",
        "    As a practice, we will NOT allow to use torch.relu and torch.nn ops\n",
        "    just for this time (you can use it from A3).\n",
        "\n",
        "    Inputs:\n",
        "    - params: a dictionary of PyTorch Tensor that store the weights of a model.\n",
        "      It should have following keys with shape\n",
        "          W1: First layer weights; has shape (D, H)\n",
        "          b1: First layer biases; has shape (H,)\n",
        "          W2: Second layer weights; has shape (H, C)\n",
        "          b2: Second layer biases; has shape (C,)\n",
        "    - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - scores: Tensor of shape (N, C) giving the classification scores for X\n",
        "    - hidden: Tensor of shape (N, H) giving the hidden layer representation\n",
        "      for each input value (after the ReLU).\n",
        "    \"\"\"\n",
        "    # Unpack variables from the params dictionary\n",
        "    W1, b1 = params['W1'], params['b1']\n",
        "    W2, b2 = params['W2'], params['b2']\n",
        "    N, D = X.shape\n",
        "\n",
        "    # Compute the forward pass\n",
        "    hidden = None\n",
        "    scores = None\n",
        "    def activation(z):\n",
        "      # TODO: use sigmoid function [https://en.wikipedia.org/wiki/Sigmoid_function]\n",
        "      pass\n",
        "    ############################################################################\n",
        "    # TODO: Perform the forward pass, computing the class scores for the input.#\n",
        "    # Store the result in the scores variable, which should be an tensor of    #\n",
        "    # shape (N, C).                                                            #\n",
        "    ############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "\n",
        "    return scores, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loss function + Gradients computation (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nn_forward_backward(params, X, y=None, reg=0.0):\n",
        "    \"\"\"\n",
        "    Compute the loss and gradients for a two layer fully connected neural\n",
        "    network. When you implement loss and gradient, please don't forget to\n",
        "    scale the losses/gradients by the batch size.\n",
        "\n",
        "    Inputs: First two parameters (params, X) are same as nn_forward_pass\n",
        "    - params: a dictionary of PyTorch Tensor that store the weights of a model.\n",
        "      It should have following keys with shape\n",
        "          W1: First layer weights; has shape (D, H)\n",
        "          b1: First layer biases; has shape (H,)\n",
        "          W2: Second layer weights; has shape (H, C)\n",
        "          b2: Second layer biases; has shape (C,)\n",
        "    - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
        "    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
        "      an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
        "      is not passed then we only return scores, and if it is passed then we\n",
        "      instead return the loss and gradients.\n",
        "    - reg: Regularization strength.\n",
        "\n",
        "    Returns:\n",
        "    If y is None, return a tensor scores of shape (N, C) where scores[i, c] is\n",
        "    the score for class c on input X[i].\n",
        "\n",
        "    If y is not None, instead return a tuple of:\n",
        "    - loss: Loss (data loss and regularization loss) for this batch of training\n",
        "      samples.\n",
        "    - grads: Dictionary mapping parameter names to gradients of those parameters\n",
        "      with respect to the loss function; has the same keys as self.params.\n",
        "    \"\"\"\n",
        "    # Unpack variables from the params dictionary\n",
        "    W1, b1 = params['W1'], params['b1']\n",
        "    W2, b2 = params['W2'], params['b2']\n",
        "    N, D = X.shape\n",
        "\n",
        "    scores, hidden = nn_forward_pass(params, X)\n",
        "    # If the targets are not given then jump out, we're done\n",
        "    if y is None:\n",
        "      return scores\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = None\n",
        "    ############################################################################\n",
        "    # TODO: Compute the loss, based on the results from nn_forward_pass.       #\n",
        "    # This should include both the data loss and L2 regularization for W1 and  #\n",
        "    # W2. Store the result in the variable loss, which should be a scalar. Use #\n",
        "    # the Cross-entropy classifier loss.                                       #\n",
        "    # Please DO NOT multiply the regularization term by 1/2 (no coefficient).  #\n",
        "    # If you are not careful here, it is easy to run into numeric instability  #\n",
        "    # (Check Numeric Stability in http://cs231n.github.io/linear-classify/).   #\n",
        "    ############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    # Backward pass: compute gradients\n",
        "    grads = {}\n",
        "    ###########################################################################\n",
        "    # TODO: Compute the backward pass, computing the derivatives of the       #\n",
        "    # weights and biases. Store the results in the grads dictionary.          #\n",
        "    # For example, grads['W1'] should store the gradient on W1, and be a      #\n",
        "    # tensor of same size                                                     #\n",
        "    ###########################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "\n",
        "    return loss, grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Weight updates (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nn_train(params, loss_func, pred_func, X, y, X_val, y_val,\n",
        "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
        "            reg=5e-6, num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "  \"\"\"\n",
        "  Train this neural network using stochastic gradient descent.\n",
        "\n",
        "  Inputs:\n",
        "  - params: a dictionary of PyTorch Tensor that store the weights of a model.\n",
        "    It should have following keys with shape\n",
        "        W1: First layer weights; has shape (D, H)\n",
        "        b1: First layer biases; has shape (H,)\n",
        "        W2: Second layer weights; has shape (H, C)\n",
        "        b2: Second layer biases; has shape (C,)\n",
        "  - loss_func: a loss function that computes the loss and the gradients.\n",
        "    It takes as input:\n",
        "    - params: Same as input to nn_train\n",
        "    - X_batch: A minibatch of inputs of shape (B, D)\n",
        "    - y_batch: Ground-truth labels for X_batch\n",
        "    - reg: Same as input to nn_train\n",
        "    And it returns a tuple of:\n",
        "      - loss: Scalar giving the loss on the minibatch\n",
        "      - grads: Dictionary mapping parameter names to gradients of the loss with\n",
        "        respect to the corresponding parameter.\n",
        "  - pred_func: prediction function that im\n",
        "  - X: A PyTorch tensor of shape (N, D) giving training data.\n",
        "  - y: A PyTorch tensor f shape (N,) giving training labels; y[i] = c means that\n",
        "    X[i] has label c, where 0 <= c < C.\n",
        "  - X_val: A PyTorch tensor of shape (N_val, D) giving validation data.\n",
        "  - y_val: A PyTorch tensor of shape (N_val,) giving validation labels.\n",
        "  - learning_rate: Scalar giving learning rate for optimization.\n",
        "  - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
        "    after each epoch.\n",
        "  - reg: Scalar giving regularization strength.\n",
        "  - num_iters: Number of steps to take when optimizing.\n",
        "  - batch_size: Number of training examples to use per step.\n",
        "  - verbose: boolean; if true print progress during optimization.\n",
        "\n",
        "  Returns: A dictionary giving statistics about the training process\n",
        "  \"\"\"\n",
        "  num_train = X.shape[0]\n",
        "  iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "\n",
        "  # Use SGD to optimize the parameters in self.model\n",
        "  loss_history = []\n",
        "  train_acc_history = []\n",
        "  val_acc_history = []\n",
        "\n",
        "  for it in range(num_iters):\n",
        "    indices = torch.randint(num_train, (batch_size,))\n",
        "    y_batch = y[indices]\n",
        "    X_batch = X[indices]\n",
        "\n",
        "    # Compute loss and gradients using the current minibatch\n",
        "    loss, grads = loss_func(params, X_batch, y=y_batch, reg=reg)\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    #########################################################################\n",
        "    # TODO: Use the gradients in the grads dictionary to update the         #\n",
        "    # parameters of the network (stored in the dictionary self.params)      #\n",
        "    # using stochastic gradient descent. You'll need to use the gradients   #\n",
        "    # stored in the grads dictionary defined above.                         #\n",
        "    #########################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    #########################################################################\n",
        "    #                             END OF YOUR CODE                          #\n",
        "    #########################################################################\n",
        "\n",
        "    if verbose and it % 100 == 0:\n",
        "      print('iteration %d / %d: loss %f' % (it, num_iters, loss.item()))\n",
        "\n",
        "    # Every epoch, check train and val accuracy and decay learning rate.\n",
        "    if it % iterations_per_epoch == 0:\n",
        "      # Check accuracy\n",
        "      y_train_pred = pred_func(params, loss_func, X_batch)\n",
        "      train_acc = (y_train_pred == y_batch).float().mean().item()\n",
        "      y_val_pred = pred_func(params, loss_func, X_val)\n",
        "      val_acc = (y_val_pred == y_val).float().mean().item()\n",
        "      train_acc_history.append(train_acc)\n",
        "      val_acc_history.append(val_acc)\n",
        "\n",
        "      # Decay learning rate\n",
        "      learning_rate *= learning_rate_decay\n",
        "\n",
        "  return {\n",
        "    'loss_history': loss_history,\n",
        "    'train_acc_history': train_acc_history,\n",
        "    'val_acc_history': val_acc_history,\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predict function (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nn_predict(params, loss_func, X):\n",
        "  \"\"\"\n",
        "  Use the trained weights of this two-layer network to predict labels for\n",
        "  data points. For each data point we predict scores for each of the C\n",
        "  classes, and assign each data point to the class with the highest score.\n",
        "\n",
        "  Inputs:\n",
        "  - params: a dictionary of PyTorch Tensor that store the weights of a model.\n",
        "    It should have following keys with shape\n",
        "        W1: First layer weights; has shape (D, H)\n",
        "        b1: First layer biases; has shape (H,)\n",
        "        W2: Second layer weights; has shape (H, C)\n",
        "        b2: Second layer biases; has shape (C,)\n",
        "  - loss_func: a loss function that computes the loss and the gradients\n",
        "  - X: A PyTorch tensor of shape (N, D) giving N D-dimensional data points to\n",
        "    classify.\n",
        "\n",
        "  Returns:\n",
        "  - y_pred: A PyTorch tensor of shape (N,) giving predicted labels for each of\n",
        "    the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
        "    to have class c, where 0 <= c < C.\n",
        "  \"\"\"\n",
        "  y_pred = None\n",
        "\n",
        "  ###########################################################################\n",
        "  # TODO: Implement this function; it should be VERY simple!                #\n",
        "  ###########################################################################\n",
        "  # Replace \"pass\" statement with your code\n",
        "  pass\n",
        "  ###########################################################################\n",
        "  #                              END OF YOUR CODE                           #\n",
        "  ###########################################################################\n",
        "\n",
        "  return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualization(stats):\n",
        "    print('Final training loss: ', stats['loss_history'][-1])\n",
        "\n",
        "    # plot the loss history\n",
        "    plt.plot(stats['loss_history'])\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('training loss')\n",
        "    plt.title('Training Loss history')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot the loss function and train / validation accuracies\n",
        "    plt.plot(stats['train_acc_history'], 'o', label='train')\n",
        "    plt.plot(stats['val_acc_history'], 'o', label='val')\n",
        "    plt.title('Classification accuracy history')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Clasification accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can test our implementation of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = TwoLayerNet(input_size=X_train.shape[1], hidden_size=128, output_size=10, device='cpu')\n",
        "tic = time.time()\n",
        "stats = model.train(X_train, y_train, X_val, y_val, verbose=False, num_iters=10000)\n",
        "toc = time.time()\n",
        "print('That took %fs' % (toc - tic))\n",
        "visualization(stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters tuning (10 pts)\n",
        "\n",
        "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, and regularization strength. You might also consider tuning other parameters such as num_iters as well.\n",
        "\n",
        "**Approximate results**. To get full credit for the assignment, you should achieve a classification accuracy above 50% on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}\n",
        "best_val = -1\n",
        "best_nn = None\n",
        "################################################################################\n",
        "# TODO (10 pts):                                                               #\n",
        "# Use the validation set to set the learning rate and regularization strength. #\n",
        "# This should be identical to the validation that you did for the SVM; save    #\n",
        "# the best trained softmax classifer in best_softmax.                          #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# fill in your own values\n",
        "learning_rates = []\n",
        "regularization_strengths = []\n",
        "hidden_dims = []\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# Print out results.\n",
        "for lr, reg, H in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg, H)]\n",
        "    print('lr %e reg %e H %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, H, train_accuracy, val_accuracy))\n",
        "\n",
        "print('best validation accuracy achieved: %f' % best_val)\n",
        "\n",
        "y_test_pred = best_nn.predict(X_test)\n",
        "test_acc = (y_test_pred == y_test).double().mean().item()\n",
        "print('final test accuracy 2-layered neural network achieved: %f' % test_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkodX4KYRcEA"
      },
      "source": [
        "# Acknowledgement\n",
        "\n",
        "Credits to [UMichigan's 498/598 Deep Learning for Computer Vision](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/) and Stanfords's [CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/), some code is adapted from their courses's assignments."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Sp9dU9BxtSsd"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
